{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bd62ff-a1e2-4d61-9540-ded621f2ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cf18de-e1dd-4e1b-b07e-358ff531117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f697ef30-3bf8-49b1-b2d2-50b5b4ac0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    \n",
    "    def __init__(self,kernel_num,kernel_size):\n",
    "        \n",
    "        self.kernel_num=kernel_num\n",
    "        self.kernel_size=kernel_size\n",
    "        self.kernels=np.random.randn(kernel_num,kernel_size,kernel_size)/(kernel_size**2)\n",
    "        \n",
    "    def patches_generator(self,image):\n",
    "        \n",
    "        image_h, image_w = image.shape\n",
    "        self.image=image\n",
    "        \n",
    "        for h in range(image_h-self.kernel_size+1):\n",
    "            for w in range(image_w-self.kernel_size+1):\n",
    "                patch=image[h:(h+self.kernel_size),w:(w+self.kernel_size)]\n",
    "\n",
    "                yield patch,h,w\n",
    "                \n",
    "    def forward_prop(self,image):\n",
    "        \n",
    "        image_h, image_w = image.shape\n",
    "        \n",
    "        convolution_output=np.zeros((image_h-self.kernel_size+1,image_w-self.kernel_size+1,self.kernel_num))\n",
    "        \n",
    "        for patch,h,w in self.patches_generator(image):\n",
    "            \n",
    "            convolution_output[h,w]=np.sum(patch*self.kernels,axis=(1,2))\n",
    "            \n",
    "        return convolution_output\n",
    "    \n",
    "    def back_prop(self,dE_dY,alpha):\n",
    "        \n",
    "        dE_dk=np.zeros(self.kernels.shape)\n",
    "        \n",
    "        for patch,h,w in self.patches_generator(self.image):\n",
    "            for f in range(self.kernel_num):\n",
    "                \n",
    "                dE_dk[f]+=patch*dE_dY[h,w,f]\n",
    "                \n",
    "        self.kernels-=alpha*dE_dk\n",
    "        \n",
    "        return dE_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f69d5224-f667-4ce1-83fa-b1cbee0c0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    \n",
    "    def __init__(self,kernel_size):\n",
    "        \n",
    "        self.kernel_size=kernel_size\n",
    "        \n",
    "        \n",
    "    def patches_generator(self,image):\n",
    "        \n",
    "        output_h=image.shape[0]//self.kernel_size\n",
    "        output_w=image.shape[1]//self.kernel_size\n",
    "        self.image=image\n",
    "        \n",
    "        for h in range(output_h):\n",
    "            for w in range(output_w):\n",
    "                \n",
    "                patch=image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size),(w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
    "                \n",
    "                yield patch, h ,w\n",
    "                \n",
    "    def forward_prop(self,image):\n",
    "        \n",
    "        image_h,image_w, num_kernels= image.shape\n",
    "        max_pooling_output= np.zeros((image_h//self.kernel_size,image_w//self.kernel_size,num_kernels))\n",
    "        \n",
    "        for patch,h,w in self.patches_generator(image):  \n",
    "            max_pooling_output[h,w]=np.amax(patch,axis=(0,1))\n",
    "            \n",
    "        return max_pooling_output\n",
    "            \n",
    "    def back_prop(self,dE_dY):\n",
    "        \n",
    "        dE_dk=np.zeros(self.image.shape)\n",
    "        \n",
    "        for patch,h,w in self.patches_generator(self.image):\n",
    "            \n",
    "            image_h,image_w,num_kernels=patch.shape\n",
    "            max_val=np.amax(patch,axis=(0,1))\n",
    "            \n",
    "            for idx_h in range(image_h):\n",
    "                for idx_w in range(image_w):\n",
    "                    for idx_k in range(num_kernels):\n",
    "                        \n",
    "                        if patch[idx_h,idx_w,idx_k]==max_val[idx_k]:\n",
    "                            dE_dk[h*self.kernel_size+idx_h,w*self.kernel_size+idx_w,idx_k]=dE_dY[h,w,idx_k]\n",
    "                            \n",
    "            return dE_dk\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44b109da-8be2-416f-a824-0b20700d757f",
   "metadata": {},
   "outputs": [],
   "source": [
    " class SoftmaxLayer:\n",
    "        \n",
    "        def __init__(self,input_units,output_units):\n",
    "            \n",
    "            self.weight=np.random.randn(input_units,output_units)/input_units\n",
    "            self.bias=np.zeros(output_units)\n",
    "            \n",
    "        def forward_prop(self,image):\n",
    "            \n",
    "            self.original_shape=image.shape  \n",
    "            image_flattened=image.flatten()\n",
    "            self.flattened_input=image_flattened\n",
    "            \n",
    "            first_output=np.dot(image_flattened,self.weight)+self.bias\n",
    "            self.output=first_output\n",
    "            \n",
    "            softmax_output=np.exp(first_output)/np.sum(np.exp(first_output),axis=0)\n",
    "            \n",
    "            return softmax_output\n",
    "        \n",
    "        def back_prop(self,dE_dY,alpha):\n",
    "            \n",
    "            for i,gradient in enumerate(dE_dY):\n",
    "                if gradient==0:\n",
    "                    continue\n",
    "                    \n",
    "                transformation_eq=np.exp(self.output)\n",
    "                S_total=np.sum(transformation_eq)\n",
    "                \n",
    "                dY_dZ= - transformation_eq[i]*transformation_eq/(S_total**2)\n",
    "                dY_dZ[i]=transformation_eq[i]*(S_total-transformation_eq[i])/(S_total**2)\n",
    "                \n",
    "                dZ_dw=self.flattened_input\n",
    "                dZ_db=1\n",
    "                dZ_dX=self.weight\n",
    "                \n",
    "                dE_dZ=gradient*dY_dZ\n",
    "                \n",
    "                dE_dw=dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
    "                \n",
    "                dE_db=dE_dZ*dZ_db\n",
    "                dE_dX=dZ_dX @ dE_dZ\n",
    "                \n",
    "                self.weight-=alpha*dE_dw\n",
    "                self.bias-= alpha*dE_db\n",
    "                \n",
    "                return dE_dX.reshape(self.original_shape)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd566ac3-0fc0-40aa-a22b-79d9599d5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_forward(image,label,layers):\n",
    "    \n",
    "    output=image/255\n",
    "    for layer in layers:\n",
    "        output=layer.forward_prop(output)\n",
    "        \n",
    "    loss = -np.log(output[label])\n",
    "    accuracy= 1 if np.argmax(output)==label else 0\n",
    "    \n",
    "    return output, loss , accuracy\n",
    "\n",
    "def CNN_backprop(gradient,layers,alpha=0.05):\n",
    "    \n",
    "    grad_back=gradient\n",
    "    \n",
    "    for layer in layers[::-1]:\n",
    "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
    "            grad_back = layer.back_prop(grad_back, alpha)\n",
    "        elif type(layer) == MaxPoolingLayer:\n",
    "            grad_back = layer.back_prop(grad_back)\n",
    "    return grad_back\n",
    "\n",
    "def CNN_training(image,label,layers,alpha=0.05):\n",
    "\n",
    "    output,loss,accuracy = CNN_forward(image,label,layers)\n",
    "    gradient=np.zeros(10)\n",
    "    gradient[label]= -1/output[label]\n",
    "    \n",
    "    gradient_back=CNN_backprop(gradient,layers,alpha)\n",
    "    \n",
    "    return loss , accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a46c673e-e717-4049-af36-1d0f573cdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = df_train.iloc[40,:].values[1:]\n",
    "img = np.reshape(img,(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b75118ed-82d2-4a1e-8a90-7eef9da1a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42850419-e5b8-4d94-b6d9-09476aab8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv = ConvolutionLayer(32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e169387-9eb3-4fe9-b957-ba556d7d145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23f22fb8be0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = my_conv.forward_prop(img)\n",
    "plt.imshow(output[:,:,15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22f576ce-6582-40d5-9d3e-41687c8b8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    X_train = X_train[:5000]\n",
    "    y_train = y_train[:5000]\n",
    "\n",
    "    layers = [\n",
    "    ConvolutionLayer(16,3), # layer with 8 3x3 filters, output (26,26,16)\n",
    "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
    "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
    "    ] \n",
    "\n",
    "    for epoch in range(4):\n",
    "        print('Epoch {} ->'.format(epoch+1))\n",
    "        # Shuffle training data\n",
    "        permutation = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[permutation]\n",
    "        y_train = y_train[permutation]\n",
    "        # Training the CNN\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "            if i % 100 == 0: # Every 100 examples\n",
    "                print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "                loss = 0\n",
    "                accuracy = 0\n",
    "            loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
    "            loss += loss_1\n",
    "            accuracy += accuracy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1146f-0ab5-428c-a660-9fd47b85e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 1.6973917421220577, accuracy 52\n",
      "Step 201. For the last 100 steps: average loss 1.0024078522556268, accuracy 79\n",
      "Step 301. For the last 100 steps: average loss 0.8880129057378323, accuracy 76\n",
      "Step 401. For the last 100 steps: average loss 0.7113435716250008, accuracy 82\n",
      "Step 501. For the last 100 steps: average loss 0.6219308902811744, accuracy 87\n",
      "Step 601. For the last 100 steps: average loss 0.7736971079373202, accuracy 79\n",
      "Step 701. For the last 100 steps: average loss 0.533159403462154, accuracy 86\n",
      "Step 801. For the last 100 steps: average loss 0.6946949000687703, accuracy 78\n",
      "Step 901. For the last 100 steps: average loss 0.3902279956804972, accuracy 90\n",
      "Step 1001. For the last 100 steps: average loss 0.6255891259184116, accuracy 82\n",
      "Step 1101. For the last 100 steps: average loss 0.43879498263719896, accuracy 86\n",
      "Step 1201. For the last 100 steps: average loss 0.5218216758747034, accuracy 82\n",
      "Step 1301. For the last 100 steps: average loss 0.5327682934308712, accuracy 83\n",
      "Step 1401. For the last 100 steps: average loss 0.4923400635619635, accuracy 85\n",
      "Step 1501. For the last 100 steps: average loss 0.4952186588976841, accuracy 86\n",
      "Step 1601. For the last 100 steps: average loss 0.47398372884972806, accuracy 86\n",
      "Step 1701. For the last 100 steps: average loss 0.39648112336927005, accuracy 90\n",
      "Step 1801. For the last 100 steps: average loss 0.3805797971115329, accuracy 91\n",
      "Step 1901. For the last 100 steps: average loss 0.49968511447580305, accuracy 84\n",
      "Step 2001. For the last 100 steps: average loss 0.4843827310495888, accuracy 85\n",
      "Step 2101. For the last 100 steps: average loss 0.40223955290193897, accuracy 88\n",
      "Step 2201. For the last 100 steps: average loss 0.47475899854608455, accuracy 86\n",
      "Step 2301. For the last 100 steps: average loss 0.5262576558951315, accuracy 86\n",
      "Step 2401. For the last 100 steps: average loss 0.3882751434631046, accuracy 91\n",
      "Step 2501. For the last 100 steps: average loss 0.3913738975058241, accuracy 87\n",
      "Step 2601. For the last 100 steps: average loss 0.6234191230245495, accuracy 83\n",
      "Step 2701. For the last 100 steps: average loss 0.31312138507224385, accuracy 90\n",
      "Step 2801. For the last 100 steps: average loss 0.3966424775895797, accuracy 88\n",
      "Step 2901. For the last 100 steps: average loss 0.2822715792902505, accuracy 90\n",
      "Step 3001. For the last 100 steps: average loss 0.347222515371416, accuracy 91\n",
      "Step 3101. For the last 100 steps: average loss 0.4785688529902102, accuracy 90\n",
      "Step 3201. For the last 100 steps: average loss 0.3815503068973293, accuracy 91\n",
      "Step 3301. For the last 100 steps: average loss 0.2845672016787841, accuracy 92\n",
      "Step 3401. For the last 100 steps: average loss 0.3864904623808861, accuracy 89\n",
      "Step 3501. For the last 100 steps: average loss 0.37558628611552014, accuracy 92\n",
      "Step 3601. For the last 100 steps: average loss 0.3730406349601069, accuracy 88\n",
      "Step 3701. For the last 100 steps: average loss 0.27669420549301077, accuracy 91\n",
      "Step 3801. For the last 100 steps: average loss 0.398894778740156, accuracy 88\n",
      "Step 3901. For the last 100 steps: average loss 0.41607456865432846, accuracy 87\n",
      "Step 4001. For the last 100 steps: average loss 0.40431359828363184, accuracy 93\n",
      "Step 4101. For the last 100 steps: average loss 0.5411999231904717, accuracy 84\n",
      "Step 4201. For the last 100 steps: average loss 0.23779282020502207, accuracy 92\n",
      "Step 4301. For the last 100 steps: average loss 0.20628995492579782, accuracy 97\n",
      "Step 4401. For the last 100 steps: average loss 0.2388167165818296, accuracy 95\n",
      "Step 4501. For the last 100 steps: average loss 0.3669379766844696, accuracy 88\n",
      "Step 4601. For the last 100 steps: average loss 0.2957509472532174, accuracy 95\n",
      "Step 4701. For the last 100 steps: average loss 0.4337284995251433, accuracy 91\n",
      "Step 4801. For the last 100 steps: average loss 0.32098636099112743, accuracy 92\n",
      "Step 4901. For the last 100 steps: average loss 0.2687674419864881, accuracy 92\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.27572639018008993, accuracy 90\n",
      "Step 201. For the last 100 steps: average loss 0.19684585858013687, accuracy 97\n",
      "Step 301. For the last 100 steps: average loss 0.20667136752111545, accuracy 95\n",
      "Step 401. For the last 100 steps: average loss 0.33526021489556923, accuracy 91\n",
      "Step 501. For the last 100 steps: average loss 0.3087346386203016, accuracy 91\n",
      "Step 601. For the last 100 steps: average loss 0.3423047094973893, accuracy 90\n",
      "Step 701. For the last 100 steps: average loss 0.2411517127295055, accuracy 92\n",
      "Step 801. For the last 100 steps: average loss 0.18791351799218384, accuracy 94\n",
      "Step 901. For the last 100 steps: average loss 0.3042377656454092, accuracy 91\n",
      "Step 1001. For the last 100 steps: average loss 0.2349154388873216, accuracy 92\n",
      "Step 1101. For the last 100 steps: average loss 0.19802665861550398, accuracy 96\n",
      "Step 1201. For the last 100 steps: average loss 0.3944229186769975, accuracy 90\n",
      "Step 1301. For the last 100 steps: average loss 0.18128329287216868, accuracy 94\n",
      "Step 1401. For the last 100 steps: average loss 0.20655210209529906, accuracy 97\n",
      "Step 1501. For the last 100 steps: average loss 0.4003997591737675, accuracy 91\n",
      "Step 1601. For the last 100 steps: average loss 0.27783429996741005, accuracy 94\n",
      "Step 1701. For the last 100 steps: average loss 0.2510097842882429, accuracy 92\n",
      "Step 1801. For the last 100 steps: average loss 0.2120620989461707, accuracy 92\n",
      "Step 1901. For the last 100 steps: average loss 0.18863099019606402, accuracy 97\n",
      "Step 2001. For the last 100 steps: average loss 0.3209808937129226, accuracy 90\n",
      "Step 2101. For the last 100 steps: average loss 0.25829570300887567, accuracy 94\n",
      "Step 2201. For the last 100 steps: average loss 0.18785259033311139, accuracy 94\n",
      "Step 2301. For the last 100 steps: average loss 0.20765535936078486, accuracy 94\n",
      "Step 2401. For the last 100 steps: average loss 0.3490973843733701, accuracy 90\n",
      "Step 2501. For the last 100 steps: average loss 0.20503036068102953, accuracy 93\n",
      "Step 2601. For the last 100 steps: average loss 0.32402517969036465, accuracy 90\n",
      "Step 2701. For the last 100 steps: average loss 0.35275859743010535, accuracy 89\n",
      "Step 2801. For the last 100 steps: average loss 0.2964611409774387, accuracy 93\n",
      "Step 2901. For the last 100 steps: average loss 0.22348815161664537, accuracy 91\n",
      "Step 3001. For the last 100 steps: average loss 0.2068300517311318, accuracy 94\n",
      "Step 3101. For the last 100 steps: average loss 0.3049362889240094, accuracy 92\n",
      "Step 3201. For the last 100 steps: average loss 0.24518053177454355, accuracy 91\n",
      "Step 3301. For the last 100 steps: average loss 0.2723314844588429, accuracy 93\n",
      "Step 3401. For the last 100 steps: average loss 0.35522787082528096, accuracy 89\n",
      "Step 3501. For the last 100 steps: average loss 0.4043336980303792, accuracy 86\n",
      "Step 3601. For the last 100 steps: average loss 0.29085006910419403, accuracy 92\n",
      "Step 3701. For the last 100 steps: average loss 0.24040996027387887, accuracy 96\n",
      "Step 3801. For the last 100 steps: average loss 0.21985020280049258, accuracy 93\n",
      "Step 3901. For the last 100 steps: average loss 0.2654240286151852, accuracy 92\n",
      "Step 4001. For the last 100 steps: average loss 0.3502338280236328, accuracy 93\n",
      "Step 4101. For the last 100 steps: average loss 0.2709098593613228, accuracy 95\n",
      "Step 4201. For the last 100 steps: average loss 0.3403126975662137, accuracy 92\n",
      "Step 4301. For the last 100 steps: average loss 0.35564443953103625, accuracy 92\n",
      "Step 4401. For the last 100 steps: average loss 0.3643057464009319, accuracy 92\n",
      "Step 4501. For the last 100 steps: average loss 0.42188072710030716, accuracy 88\n",
      "Step 4601. For the last 100 steps: average loss 0.37715585561216175, accuracy 91\n",
      "Step 4701. For the last 100 steps: average loss 0.3573774384576556, accuracy 88\n",
      "Step 4801. For the last 100 steps: average loss 0.15987677724467744, accuracy 97\n",
      "Step 4901. For the last 100 steps: average loss 0.22346065122344963, accuracy 95\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac2590-421b-4f7b-83ce-873b7ff6d5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
